# phdgfl_builder/builder.py
# -*- coding: utf-8 -*-
import os
import csv
import multiprocessing as mp
from pathlib import Path
from typing import List, Dict

from tqdm import tqdm

from .log_utils import setup_logger
from .monitor import PerformanceMonitor
from .discover import discover_tasks
from .worker import process_one_project
from .encoder import CodeBERTEncoder


# âœ… å…³é”®ï¼šå¿…é¡»æ˜¯â€œæ¨¡å—é¡¶å±‚å‡½æ•°â€ï¼Œspawn æ‰èƒ½ pickle
def pool_initializer(cfg: dict):
    CodeBERTEncoder.init(cfg, logger=None)


def run_build(cfg: dict, mp_start_method: str = "spawn"):
    os.environ.setdefault("HF_HUB_OFFLINE", "1")
    os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")

    logger = setup_logger("build-all", cfg.get("log_level", "INFO"))

    in_root = Path(cfg["input_root_base"])
    out_root = Path(cfg["output_root_base"])
    out_root.mkdir(parents=True, exist_ok=True)

    tasks = discover_tasks(in_root, cfg)
    if not tasks:
        logger.error(f"æœªå‘ç°ä»»ä½•å¯å¤„ç†ä»»åŠ¡ï¼š{in_root}")
        return

    worker_args = [(ds, pid, str(pdir), cfg, cfg.get("log_level", "INFO")) for (ds, pid, pdir) in tasks]

    monitor = PerformanceMonitor()

    try:
        mp.set_start_method(mp_start_method, force=True)
    except RuntimeError:
        pass

    ctx = mp.get_context(mp_start_method)

    all_stats: List[Dict] = []
    nw = int(cfg["num_workers"])

    logger.info(f"è¾“å…¥: {in_root}")
    logger.info(f"è¾“å‡º: {out_root}")
    logger.info(
        f"ä»»åŠ¡æ•°(é¡¹ç›®): {len(worker_args)} | workers={nw} | save_mode={cfg['save_mode']} | "
        f"feature_mode={cfg['feature_mode']} | edge_mode={cfg['edge_build_mode']}"
    )

    # âœ… æ³¨æ„è¿™é‡Œï¼šinitializer ç”¨é¡¶å±‚å‡½æ•° + initargs ä¼  cfg
    with ctx.Pool(processes=nw, initializer=pool_initializer, initargs=(cfg,)) as pool:
        for st in tqdm(pool.imap_unordered(process_one_project, worker_args),
                       total=len(worker_args), desc="ğŸš€ æ„å»ºå›¾æ•°æ®", unit="proj"):
            monitor.update(st["dataset"], st["pid"], st["graphs"], st["nodes"], st["edges"], st["duration"])
            all_stats.append(st)

    monitor.report()
    print(f"\næ•°æ®å·²ä¿å­˜åˆ°: {cfg['output_root_base']}")
    print(f"ä¿å­˜æ¨¡å¼: {cfg['save_mode']}")

    if cfg.get("export_index_csv", True):
        try:
            index_rows = []
            for st in all_stats:
                for p in st.get("outputs", []):
                    index_rows.append({"dataset": st["dataset"], "pid": st["pid"], "pt_path": p})

            idx_csv = out_root / "index.csv"
            with open(idx_csv, "w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=["dataset", "pid", "pt_path"])
                w.writeheader()
                w.writerows(index_rows)

            logger.info(f"ç´¢å¼•è¡¨å·²å¯¼å‡ºï¼š{idx_csv}ï¼ˆ{len(index_rows)} æ¡ï¼‰")
        except Exception as e:
            logger.warning(f"å¯¼å‡º index.csv å¤±è´¥ï¼š{e}")
